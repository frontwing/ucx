/**
* Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
*
* See file LICENSE for terms.
*/

#include "migration.h"
#include "address.h"
#include "stub_ep.h"
#include "migration.h"

#include <ucp/core/ucp_ep.h>
#include <ucp/core/ucp_request.inl>
#include <ucp/core/ucp_worker.h>
#include <ucs/arch/bitops.h>
#include <ucs/async/async.h>

static void ucp_migration_handle_standby()
{
    /* Freeze this address */

    /* Create the migration ID for this connection */

    /* Send acknowledgement */
    ucs_trace("ep %p: sending migration standby acknowledgement", ep);
    status = ucp_wireup_msg_send(ep, UCP_WIREUP_MSG_REPLY, tl_bitmap, rsc_tli);
    if (status != UCS_OK) {
        return;
    }
    ep->flags |= UCP_EP_FLAG_CONNECT_REP_SENT;
}

static ucs_status_t ucp_migration_msg_handler(void *arg, void *data,
                                           size_t length, void *desc)
{
    ucp_worker_h worker   = arg;
    ucp_migration_msg_t *msg = data;
    char peer_name[UCP_WORKER_NAME_MAX];
    ucp_address_entry_t *address_list;
    unsigned address_count;
    ucs_status_t status;
    uint64_t uuid;

    UCS_ASYNC_BLOCK(&worker->async);

    status = ucp_address_unpack(msg + 1, &uuid, peer_name, UCP_WORKER_NAME_MAX,
                                &address_count, &address_list);
    if (status != UCS_OK) {
        ucs_error("failed to unpack address: %s", ucs_status_string(status));
        goto out;
    }

    if (msg->type == UCP_MIGRATION_MSG_STANDBY) {
		/* This means I am the client and I just got a message from s1 that s1
         * is going to start migration proceedings. I need to create an ACK message
         * which has a  ID and send it back to s1. s1 sends one of these for each client 
         * conneted to it */
		// 0. Stop sending app data to s1. How?
		// 1. Create a migration/client ID. Random uint64_t. Just needs to be locally unique?
		// 2. Send the migration/client ID to s1 via STANDYBY_ACK message
		// 3. Prepare to receive a new message from s2 eventually.

    } else if (msg->type == UCP_MIGRATION_MSG_STANDBY_ACK) {
		/* This means I am the server and I just got a message from client ACKing my
         * request to migrate client to s2. The ack message has the migration ID created by the client,
         * which needs forwarded to s2, along with client information (address_stuff) */
		// 1. Foreach(client) -> Gather (client address_stuff) plus client ID and pack into single message 
		// 2. Foreach(client) -> Send packed message to s2. each message will have the total number of clients
		// 	(wasted space,but fine for now)
		// 3. Prepare for the migration_complete message eventaully.

    } else if (msg->type == UCP_MIGRATION_MSG_MIGRATE) {
		/* This means I am S2 and I am getting a client ID from s1 (which was generated by the client). 
         * s1 should have packed the client information I need to establish a connection with the client
         * in this message. I will now contact each client (using information from S1) */
		/* There is only one client for each single migrate message, but this is what happens: */
		// 1. Foreach(client) -> Unpack (client address_stuff)
		// 2. Foreach(client) -> Establish a connection to the client based on (address_stuff)
		// 3. Foreach(client) -> Create msg_redirect msg with client ID in it and send to each client

    } else if (msg->type == UCP_MIGRATION_MSG_REDIRECT) {
		/* This means I am a client and am getting new "server" information. The new server information is
 		 * extractable from the header (doesn't need to be explicit in the payload or anything). I am also sending the
 		 * client ID in the payload. I will also prepare the redirect_ack */

    } else if (msg->type == UCP_MIGRATION_MSG_REDIRECT_ACK) {
		/* This means I am s2 and I've had a client acknowledge complete setup of the migration. I need to count
         * these and ensure I get one per expected client. As soon as I get all of the redirect_acks, I send a
         * migrate_complete to s1 */
		// 1. Foreach(client) -> mark reception of redirect message
		// 2. When complete, prepare  complete message for S1
	} else if (msg->type == UCP_MIGRATION_MSG_MIGRATE_COMPLETE) {
		/* This means I am s1 and everything has been migrated to s2. I can shut down any client connections I want */
		// 1. Free any resources used for client or S2 communications.

    } else {
        ucs_bug("invalid migration message");
    }

    ucs_free(address_list);

out:
    UCS_ASYNC_UNBLOCK(&worker->async);
    return UCS_OK;
}

ucs_status_t ucp_migration_send_request(ucp_ep_h ep)
{
//    ucp_worker_h worker = ep->worker;
//    ucp_rsc_index_t rsc_tli[UCP_MAX_LANES];
//    ucp_rsc_index_t rsc_index;
//    uint64_t tl_bitmap = 0;
//    ucp_lane_index_t lane;
    ucs_status_t status;

    if (ep->flags & UCP_EP_FLAG_CONNECT_REQ_SENT) {
        return UCS_OK;
    }

    ucs_assert_always(!ucp_ep_is_stub(ep));

//    for (lane = 0; lane < UCP_MAX_LANES; ++lane) {
//        if (lane < ucp_ep_num_lanes(ep)) {
//            rsc_index = ucp_ep_get_rsc_index(ep, lane);
//            rsc_tli[lane] = ucp_worker_is_tl_p2p(worker, rsc_index) ? rsc_index :
//                                                                      UCP_NULL_RESOURCE;
//            tl_bitmap |= UCS_BIT(rsc_index);
//        } else {
//            rsc_tli[lane] = UCP_NULL_RESOURCE;
//        }
//    }

//    /* TODO make sure such lane would exist */
//    rsc_index = ucp_stub_ep_get_aux_rsc_index(
//                    ep->uct_eps[ucp_ep_get_migration_msg_lane(ep)]);
//    if (rsc_index != UCP_NULL_RESOURCE) {
//        tl_bitmap |= UCS_BIT(rsc_index);
//    }

    ucs_debug("ep %p: send migration request (flags=0x%x)", ep, ep->flags);
//    status = ucp_migration_msg_send(ep, UCP_MIGRATION_MSG_REQUEST, tl_bitmap, rsc_tli);
    ep->flags |= UCP_EP_FLAG_CONNECT_REQ_SENT;
    return status;
}

struct migration_context {
    char clients_acked[MAX_CLIENTS];
};

static int send_standby_to_ep()
{
    status = ucp_migration_msg_send(ep, UCP_MIGRATION_MSG_STANDBY);
}

ucs_status_t ucp_worker_migrate(ucp_worker_h worker, ucp_ep_h target)
{
    struct migration_context this_migration = {0};

    /* Send all the clients STANDBY */
    ucp_ep_h iterator;
    kh_foreach_value(worker->ep_hash, iterator, send_standby_to_ep);

    /* Waits for ACKs */
    struct migration_context *ctx;
    while (!all_acked(ctx->clients_acked)) {
        progress();
    }

    /* Sends <target> the peer addresses */
    kh_foreach_value(worker->ep_hash, iterator, get_ep_address_stuff);
    status = ucp_migration_msg_send(ep, UCP_MIGRATION_MSG_MIGRATE, address_stuff);

    /* */
    while (!target_acked) {
        progress();
    }

    return UCS_OK;
}

static void ucp_migration_msg_dump(ucp_worker_h worker, uct_am_trace_type_t type,
                                uint8_t id, const void *data, size_t length,
                                char *buffer, size_t max)
{
    ucp_context_h context       = worker->context;
    const ucp_migration_msg_t *msg = data;
    char peer_name[UCP_WORKER_NAME_MAX + 1];
    ucp_address_entry_t *address_list, *ae;
    ucp_tl_resource_desc_t *rsc;
    unsigned address_count;
//    ucp_lane_index_t lane;
    uint64_t uuid;
    char *p, *end;

    ucp_address_unpack(msg + 1, &uuid, peer_name, sizeof(peer_name),
                       &address_count, &address_list);

    p   = buffer;
    end = buffer + max;
    snprintf(p, end - p, "MIGRATION %s [%s uuid 0x%"PRIx64"]",
             (msg->type == UCP_MIGRATION_MSG_STANDBY     ) ? "STDBY" :
             (msg->type == UCP_MIGRATION_MSG_STANDBY_ACK ) ? "STACK" :
             (msg->type == UCP_MIGRATION_MSG_DESTINATION ) ? "DEST" :
             (msg->type == UCP_MIGRATION_MSG_REDIRECT    ) ? "RDRCT" : "",
             peer_name, uuid);

    p += strlen(p);
    for (ae = address_list; ae < address_list + address_count; ++ae) {
        for (rsc = context->tl_rscs; rsc < context->tl_rscs + context->num_tls; ++rsc) {
            if (ae->tl_name_csum == rsc->tl_name_csum) {
                snprintf(p, end - p, " "UCT_TL_RESOURCE_DESC_FMT,
                         UCT_TL_RESOURCE_DESC_ARG(&rsc->tl_rsc));
                p += strlen(p);
                break;
            }
        }
        snprintf(p, end - p, "/md[%d]", ae->md_index);
        p += strlen(p);

//        for (lane = 0; lane < UCP_MAX_LANES; ++lane) {
//            if (msg->tli[lane] == (ae - address_list)) {
//                snprintf(p, end - p, "/lane[%d]", lane);
//                p += strlen(p);
//            }
//        }
    }

    ucs_free(address_list);
}
/*
 * @param [in] rsc_tli  Resource index for every lane.
 */
static ucs_status_t ucp_migration_msg_send(ucp_ep_h ep, uint8_t type,
                                        uint64_t tl_bitmap,
                                        const ucp_rsc_index_t *rsc_tli)
{
    ucp_rsc_index_t rsc_index;
    ucp_lane_index_t lane;
    unsigned order[UCP_MAX_LANES + 1];
    ucp_request_t* req;
    ucs_status_t status;
    void *address;

    ucs_assert(ep->cfg_index != (uint8_t)-1);

    /* We cannot allocate from memory pool because it's not thread safe
     * and this function may be called from any thread
     */
    req = ucs_malloc(sizeof(*req), "migration_msg_req");
    if (req == NULL) {
        return UCS_ERR_NO_MEMORY;
    }

    req->flags                   = 0;
    req->send.ep                 = ep;
    req->send.migration.type        = type;
    req->send.uct.func           = ucp_migration_msg_progress;
    req->send.datatype           = ucp_dt_make_contig(1);

    /* pack all addresses */
    status = ucp_address_pack(ep->worker, ep, tl_bitmap, order,
                              &req->send.length, &address);
    if (status != UCS_OK) {
        ucs_free(req);
        ucs_error("failed to pack address: %s", ucs_status_string(status));
        return status;
    }

    req->send.buffer = address;

    /* send the indices addresses that should be connected by remote side */
    for (lane = 0; lane < UCP_MAX_LANES; ++lane) {
        rsc_index = rsc_tli[lane];
        if (rsc_index != UCP_NULL_RESOURCE) {
            req->send.migration.tli[lane] = ucp_migration_address_index(order,
                                                                  tl_bitmap,
                                                                  rsc_index);
        } else {
            req->send.migration.tli[lane] = -1;
        }
    }

    ucp_request_start_send(req);
    return UCS_OK;
}

UCP_DEFINE_AM(-1, UCP_AM_ID_MIGRATION, ucp_migration_msg_handler,
              ucp_migration_msg_dump, UCT_AM_CB_FLAG_ASYNC);






